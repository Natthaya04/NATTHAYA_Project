{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natthaya04/NATTHAYA_Project/blob/main/Youtube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ติดตั้ง library เพิ่ม + เตรียมสภาพแวดล้อมสำหรับใช้ Spark + Pandas + Kaggle"
      ],
      "metadata": {
        "id": "GUI_MEvp-Zqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFVjkX_t-SL1"
      },
      "outputs": [],
      "source": [
        "#Install extra lib(s)\n",
        "!pip install -q xlrd\n",
        "!pip install -q kaggle\n",
        "!pip install -q kora\n",
        "\n",
        "import kora\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ติดตั้ง/อัปเกรด library opendatasets เพื่อใช้โหลด dataset จาก Kaggle"
      ],
      "metadata": {
        "id": "VapULj6G-kPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install opendatasets --upgrade"
      ],
      "metadata": {
        "id": "eIjfbuSB-kB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ติดตั้งไลบรารี googletrans รุ่น 4.0.0-rc1 ใช้สำหรับแปลภาษาใน Python"
      ],
      "metadata": {
        "id": "8zbAX9UAYa67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "id": "p7SrcO8FYaH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "โหลด dataset จาก Kaggle (YouTube trending videos) แล้ว เก็บไว้ใน mypath"
      ],
      "metadata": {
        "id": "OyldP3Po-z27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/datasnaek/youtube-new/data\", \"/content/mypath/\")"
      ],
      "metadata": {
        "id": "ln7-1p0y-zp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ตรวจสอบว่าในโฟลเดอร์มีไฟล์อะไรบ้าง"
      ],
      "metadata": {
        "id": "p7Po97gZ_2aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Assuming the path is the same as defined in cell Vy1GNswuqOy5\n",
        "path = \"/content/mypath/youtube-new\"\n",
        "files = os.listdir(path)\n",
        "for f in files:\n",
        "    print(f)"
      ],
      "metadata": {
        "id": "VlGt-r92_2O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# โหลดไฟล์ JSON\n",
        "with open('/content/mypath/youtube-new/CA_category_id.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)  # data is a dict\n",
        "\n",
        "# Access the list of items and print the first 5 titles\n",
        "for video in data['items'][:5]:\n",
        "    print(video['snippet']['title'])"
      ],
      "metadata": {
        "id": "nUbkxrNnxWLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# โหลด JSON\n",
        "with open('/content/mypath/youtube-new/CA_category_id.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Process the 'items' list to create a list of dictionaries with relevant data\n",
        "categories = []\n",
        "for item in data['items']:\n",
        "    category_id = item['id']\n",
        "    category_title = item['snippet']['title']\n",
        "    categories.append({'category_id': category_id, 'category_title': category_title})\n",
        "\n",
        "# แปลงเป็น DataFrame\n",
        "df_category = pd.DataFrame(categories)\n",
        "\n",
        "# ดู column\n",
        "print(df_category.columns)\n",
        "\n",
        "# ดู category ที่มี\n",
        "print(df_category['category_title'].unique())\n",
        "\n",
        "# Categoory name และ idทั้งหมด\n",
        "print(df_category)\n",
        "\n",
        "# บันทึกเป็น CSV พร้อมรองรับภาษาไทย\n",
        "df_category.to_csv(\"df_category.csv\", index=False, encoding='utf-8-sig')\n",
        "print(\"Saved df_category.csv successfully!\")\n"
      ],
      "metadata": {
        "id": "Ujsyxh-68YOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "อ่านไฟล์ CSV จาก mypath ด้วย PySpark เป็น DataFrame"
      ],
      "metadata": {
        "id": "xeKLQA7SAH4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"YouTubeAnalysis\").getOrCreate()\n",
        "\n",
        "file_path = \"/content/mypath/youtube-new\" # Corrected the file path\n",
        "\n",
        "df_input = spark.read.options(header=\"true\", inferschema=\"true\").csv(file_path)\n",
        "\n",
        "df_input.printSchema()\n",
        "\n",
        "display(df_input)"
      ],
      "metadata": {
        "id": "chq3QZ0DAHsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล United States"
      ],
      "metadata": {
        "id": "-gXxFVJSA6B-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube US Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/USvideos.csv\"\n",
        "\n",
        "# Read the CSV into df_us\n",
        "\n",
        "df_us = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_us.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_us.show(20)\n"
      ],
      "metadata": {
        "id": "40XZOW7SA55e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "US Cleaning"
      ],
      "metadata": {
        "id": "3ue-SYRpWxfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_us.count()}\")"
      ],
      "metadata": {
        "id": "sEw7wwBHcO_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Initialize Spark Session (if not already initialized)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube US Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/USvideos.csv\"\n",
        "\n",
        "# Read the CSV into df_us\n",
        "df_us = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# เลือกเฉพาะ title ที่เป็นภาษาอังกฤษ\n",
        "df_us_clean = df_us.filter(\n",
        "    col(\"title\").rlike(\"[A-Za-z]\")\n",
        ")\n",
        "# กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น (0-9)\n",
        "df_us_clean = df_us_clean.filter(\n",
        "    col(\"views\").rlike(\"^[0-9]+$\") &\n",
        "    col(\"likes\").rlike(\"^[0-9]+$\") &\n",
        "    col(\"dislikes\").rlike(\"^[0-9]+$\")\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในคอลัมน์สำคัญ\n",
        "df_us_clean = df_us_clean.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_us_clean = df_us_clean.dropDuplicates()\n",
        "\n",
        "# จัดลำดับตาม views จากมากไปน้อย\n",
        "df_us_clean = df_us_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_us_clean.show(20, truncate=False)\n",
        "\n",
        "# แปลงเป็น Pandas และบันทึกเป็น CSV\n",
        "df_us_clean = df_us_clean.toPandas()\n",
        "df_us_clean.to_csv(\"df_us_clean.csv\", index=False)\n",
        "\n",
        "# จำนวนแถวใน\n",
        "print(f\"จำนวนแถว: {df_us_clean.count()}\")"
      ],
      "metadata": {
        "id": "i6EDHUlCH5sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "เพิ่ม Weeky, Hour, Title_langth, Tags_count, Like_dislike_ratio, day_to_trending"
      ],
      "metadata": {
        "id": "EczcACZkbwWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube US Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/USvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "df_us = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_us_clean_spark = df_us.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_us_clean = df_us_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_us_clean[c] = pd.to_numeric(df_us_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_us_clean['publish_time'] = pd.to_datetime(df_us_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_us_clean['trending_date'] = pd.to_datetime(df_us_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_us_clean['title_length'] = df_us_clean['title'].astype(str).apply(len)\n",
        "df_us_clean['tags_count'] = df_us_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_us_clean['days_to_trending'] = (df_us_clean['trending_date'].dt.normalize() - df_us_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_us_clean['weekday'] = df_us_clean['publish_time'].dt.day_name()\n",
        "df_us_clean['hour'] = df_us_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_us_clean.to_csv(\"df_us_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_us_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_us_clean)}\")\n"
      ],
      "metadata": {
        "id": "pFxQ3IYklsWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล Canada\n",
        "\n"
      ],
      "metadata": {
        "id": "hJllgcLy-j5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube CA Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/CAvideos.csv\"\n",
        "\n",
        "# Read the CSV into df_ca\n",
        "df_ca = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "\n",
        "# ดู schema\n",
        "df_ca.printSchema()\n",
        "\n",
        "#แสดงตัวอย่างข้อมูล\n",
        "df_ca.show(20)"
      ],
      "metadata": {
        "id": "XllJWxlEBKlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CA Cleaning"
      ],
      "metadata": {
        "id": "mVrTg5BqXUGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_ca.count()}\")"
      ],
      "metadata": {
        "id": "53AjUjZ5cJ8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Spark Session (if not already initialized)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube CA Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/CAvideos.csv\"\n",
        "\n",
        "# Read the CSV into df_ca\n",
        "df_ca = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_ca_clean = df_ca.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_ca_clean = df_ca_clean.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_ca_clean = df_ca_clean.dropDuplicates()\n",
        "\n",
        "# จัดลำดับตาม views จากมากไปน้อย\n",
        "df_ca_clean = df_ca_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_ca_clean.show(20, truncate=False)\n",
        "\n",
        "# แปลงเป็น Pandas และบันทึกเป็น CSV\n",
        "df_ca_clean = df_ca_clean.toPandas()\n",
        "df_ca_clean.to_csv(\"df_ca_clean.csv\", index=False)\n",
        "\n",
        "# จำนวนแถวใน\n",
        "print(f\"จำนวนแถว: {df_ca_clean.count()}\")"
      ],
      "metadata": {
        "id": "tdABY5EUZ_Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube CA Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/CAvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "df_ca = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_ca_clean_spark = df_ca.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_ca_clean = df_ca_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_ca_clean[c] = pd.to_numeric(df_ca_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_ca_clean['publish_time'] = pd.to_datetime(df_ca_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_ca_clean['trending_date'] = pd.to_datetime(df_ca_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_ca_clean['title_length'] = df_ca_clean['title'].astype(str).apply(len)\n",
        "df_ca_clean['tags_count'] = df_ca_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_ca_clean['days_to_trending'] = (df_ca_clean['trending_date'].dt.normalize() - df_ca_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_ca_clean['weekday'] = df_ca_clean['publish_time'].dt.day_name()\n",
        "df_ca_clean['hour'] = df_ca_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_ca_clean.to_csv(\"df_ca_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_ca_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_ca_clean)}\")\n"
      ],
      "metadata": {
        "id": "kUv7nHhsmN8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล Germany"
      ],
      "metadata": {
        "id": "IIoYsmIgBYzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube DE Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/DEvideos.csv\"\n",
        "\n",
        "\n",
        "# Read the CSV into df_de\n",
        "\n",
        "df_de = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_de.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_de.show(20)\n"
      ],
      "metadata": {
        "id": "YC84Eky-BZfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DE Cleaning"
      ],
      "metadata": {
        "id": "harKsvTXcmvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_de.count()}\")"
      ],
      "metadata": {
        "id": "H6EVnMGHcq04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "# กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_de_clean = df_de.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_de_clean = df_de_clean.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_de_clean = df_de_clean.dropDuplicates()\n",
        "\n",
        "# จัดลำดับตาม views จากน้อยไปมาก\n",
        "df_de_clean = df_de_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_de_clean.show(20, truncate=False)\n",
        "\n",
        "# แปลงเป็น Pandas และบันทึกเป็น CSV\n",
        "df_de_clean = df_de_clean.toPandas()\n",
        "df_de_clean.to_csv(\"df_de_clean.csv\", index=False)\n",
        "\n",
        "# จำนวนแถวใน Pandas\n",
        "print(f\"จำนวนแถว: {df_de_clean.count()}\")"
      ],
      "metadata": {
        "id": "hYUN-anBeIjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube DE Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/DEvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "df_de = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_de_clean_spark = df_de.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_de_clean = df_de_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_de_clean[c] = pd.to_numeric(df_de_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_de_clean['publish_time'] = pd.to_datetime(df_de_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_de_clean['trending_date'] = pd.to_datetime(df_de_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_de_clean['title_length'] = df_de_clean['title'].astype(str).apply(len)\n",
        "df_de_clean['tags_count'] = df_de_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_de_clean['days_to_trending'] = (df_de_clean['trending_date'].dt.normalize() - df_de_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_de_clean['weekday'] = df_de_clean['publish_time'].dt.day_name()\n",
        "df_de_clean['hour'] = df_de_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_de_clean.to_csv(\"df_de_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_de_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_de_clean)}\")\n"
      ],
      "metadata": {
        "id": "VId_SkOenC7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "\n",
        "# โหลด CSV\n",
        "df_de_clean = pd.read_csv(\"/content/df_de_clean.csv\")\n",
        "\n",
        "# เลือก Top trending 100 แถว (ตาม views สูงสุด)\n",
        "df_de_clean = df_de_clean.sort_values(by='views', ascending=False).head(100)\n",
        "\n",
        "# สร้าง Translator\n",
        "translator = Translator()\n",
        "\n",
        "# ฟังก์ชันแปลภาษา พร้อมจัดการ error\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        return translator.translate(str(text), dest='en').text\n",
        "    except:\n",
        "        return str(text)\n",
        "\n",
        "# แปล title\n",
        "df_de_clean['title_en'] = df_de_clean['title'].apply(translate_text)\n",
        "\n",
        "# บันทึก CSV\n",
        "df_de_clean.to_csv(\"/content/df_de_clean.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "display(df_de_clean.head(20))"
      ],
      "metadata": {
        "id": "PLANdG31do_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล France\n"
      ],
      "metadata": {
        "id": "-hMhB27tBhej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube FR Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/FRvideos.csv\"\n",
        "\n",
        "\n",
        "# Read the CSV into df_fr\n",
        "\n",
        "df_fr = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_fr.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_fr.show(20)\n"
      ],
      "metadata": {
        "id": "X4mRg-KJBjnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FR Cleaning\n"
      ],
      "metadata": {
        "id": "OQYHdJ6Sf-8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_fr.count()}\")"
      ],
      "metadata": {
        "id": "_b2u99eNgFCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "# กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_fr_clean  = df_fr.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_fr_clean = df_fr_clean .dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_fr_clean = df_fr_clean.dropDuplicates()\n",
        "\n",
        "# จัดลำดับตาม views จากน้อยไปมาก\n",
        "df_fr_clean = df_fr_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_fr_clean.show(20, truncate=False)\n",
        "\n",
        "# แปลงเป็น Pandas และบันทึกเป็น CSV\n",
        "df_fr_clean = df_fr_clean.toPandas()\n",
        "df_fr_clean.to_csv(\"df_fr_clean.csv\", index=False)\n",
        "\n",
        "# จำนวนแถวใน Pandas\n",
        "print(f\"จำนวนแถว: {df_fr_clean.count()}\")"
      ],
      "metadata": {
        "id": "LtHiBa_CsQVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube FR Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/FRvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_fr_clean_spark = df_fr.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_fr_clean = df_fr_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_fr_clean[c] = pd.to_numeric(df_fr_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_fr_clean['publish_time'] = pd.to_datetime(df_fr_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_fr_clean['trending_date'] = pd.to_datetime(df_fr_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_fr_clean['title_length'] = df_fr_clean['title'].astype(str).apply(len)\n",
        "df_fr_clean['tags_count'] = df_fr_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_fr_clean['days_to_trending'] = (df_fr_clean['trending_date'].dt.normalize() - df_fr_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_fr_clean['weekday'] = df_fr_clean['publish_time'].dt.day_name()\n",
        "df_fr_clean['hour'] = df_fr_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_fr_clean.to_csv(\"df_fr_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_fr_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_fr_clean)}\")\n"
      ],
      "metadata": {
        "id": "8Qbpcsx1peLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล United Kingdom"
      ],
      "metadata": {
        "id": "cEmUZzedFSOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube GB Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/GBvideos.csv\"\n",
        "\n",
        "\n",
        "# Read the CSV into df_gb\n",
        "\n",
        "df_gb = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_gb.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_gb.show(20)\n"
      ],
      "metadata": {
        "id": "0b2UO8VqFR7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GB Cieaning"
      ],
      "metadata": {
        "id": "1aPsWbXIv_2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_gb.count()}\")"
      ],
      "metadata": {
        "id": "-4FpilIsv_cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "# กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_gb_clean  = df_gb.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_gb_clean = df_gb_clean.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_gb_clean = df_gb_clean.dropDuplicates()\n",
        "\n",
        "# จัดลำดับตาม views จากน้อยไปมาก\n",
        "df_gb_clean = df_gb_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_gb_clean.show(20, truncate=False)\n",
        "\n",
        "# แปลงเป็น Pandas และบันทึกเป็น CSV\n",
        "df_gb_clean = df_gb_clean.toPandas()\n",
        "df_gb_clean.to_csv(\"df_gb_clean.csv\", index=False)\n",
        "\n",
        "# จำนวนแถวใน Pandas\n",
        "print(f\"จำนวนแถว: {df_gb_clean.count()}\")"
      ],
      "metadata": {
        "id": "ayVsfW_dwHFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube GB Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/GBvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_gb_clean_spark = df_gb.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_gb_clean = df_gb_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_gb_clean[c] = pd.to_numeric(df_fr_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_gb_clean['publish_time'] = pd.to_datetime(df_gb_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_gb_clean['trending_date'] = pd.to_datetime(df_gb_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_gb_clean['title_length'] = df_gb_clean['title'].astype(str).apply(len)\n",
        "df_gb_clean['tags_count'] = df_gb_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_gb_clean['days_to_trending'] = (df_gb_clean['trending_date'].dt.normalize() - df_gb_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_gb_clean['weekday'] = df_gb_clean['publish_time'].dt.day_name()\n",
        "df_gb_clean['hour'] = df_gb_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_gb_clean.to_csv(\"df_gb_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_gb_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_gb_clean)}\")\n"
      ],
      "metadata": {
        "id": "A-l2VacCqW4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล India"
      ],
      "metadata": {
        "id": "ek2HqZXDFpZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube IN Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/INvideos.csv\"\n",
        "\n",
        "\n",
        "# Read the CSV into df_in\n",
        "\n",
        "df_in = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_in.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_in.show(20)\n"
      ],
      "metadata": {
        "id": "-hVLKNQkFuz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN Cleaning"
      ],
      "metadata": {
        "id": "ZUPzC5-RwzI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_in.count()}\")"
      ],
      "metadata": {
        "id": "x2x75g5Rw1gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "# กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_in_clean  = df_in.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_in_clean =df_in_clean.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_in_clean =df_in_clean.dropDuplicates()\n",
        "\n",
        "# จัดลำดับตาม views จากน้อยไปมาก\n",
        "df_in_clean = df_in_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_in_clean.show(20, truncate=False)\n",
        "\n",
        "# แปลงเป็น Pandas และบันทึกเป็น CSV\n",
        "df_in_clean = df_in_clean.toPandas()\n",
        "df_in_clean.to_csv(\"df_in_clean.csv\", index=False)\n",
        "\n",
        "# จำนวนแถวใน Pandas\n",
        "print(f\"จำนวนแถว: {df_in_clean.count()}\")"
      ],
      "metadata": {
        "id": "tNfuGyDDw3HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube IN Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/INvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_in_clean_spark = df_in.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_in_clean = df_in_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_in_clean[c] = pd.to_numeric(df_in_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_in_clean['publish_time'] = pd.to_datetime(df_in_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_in_clean['trending_date'] = pd.to_datetime(df_in_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_in_clean['title_length'] = df_in_clean['title'].astype(str).apply(len)\n",
        "df_in_clean['tags_count'] = df_in_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_in_clean['days_to_trending'] = (df_in_clean['trending_date'].dt.normalize() - df_in_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_in_clean['weekday'] = df_in_clean['publish_time'].dt.day_name()\n",
        "df_in_clean['hour'] = df_in_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_in_clean.to_csv(\"df_in_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_in_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_in_clean)}\")\n"
      ],
      "metadata": {
        "id": "iFpNWLUCrE4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล Japan"
      ],
      "metadata": {
        "id": "-0dfCRH1F9ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube JP Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/JPvideos.csv\"\n",
        "\n",
        "\n",
        "# Read the CSV into df_jp\n",
        "\n",
        "df_jp = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_jp.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_jp.show(20)\n"
      ],
      "metadata": {
        "id": "NECso1WEGC2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JP Cleaning\n"
      ],
      "metadata": {
        "id": "D0H5Qv0qxR07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_jp.count()}\")"
      ],
      "metadata": {
        "id": "KgEsdy3kxVon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "# กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_jp_clean = df_jp.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_jp_clean = df_jp_clean.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_jp_clean = df_jp_clean.dropDuplicates()\n",
        "\n",
        "#จัดลำดับตาม views จากมากไปน้อย\n",
        "df_jp_clean = df_jp_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_jp_clean.show(20, truncate=False)\n",
        "\n",
        "#แปลงเป็น Pandas และบันทึกเป้น CSV\n",
        "df_jp_clean = df_jp_clean.toPandas()\n",
        "df_jp_clean.to_csv(\"df_jp_clean.csv\", index=False)\n",
        "\n",
        "#แสดงจำนวนแถว\n",
        "print(f\"จำนวนแถว: {df_jp_clean.count()}\")"
      ],
      "metadata": {
        "id": "7EFggZbKxdJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube JP Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/JPvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_jp_clean_spark = df_jp.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_jp_clean = df_jp_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_jp_clean[c] = pd.to_numeric(df_jp_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_jp_clean['publish_time'] = pd.to_datetime(df_jp_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_jp_clean['trending_date'] = pd.to_datetime(df_jp_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_jp_clean['title_length'] = df_jp_clean['title'].astype(str).apply(len)\n",
        "df_jp_clean['tags_count'] = df_jp_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_jp_clean['days_to_trending'] = (df_jp_clean['trending_date'].dt.normalize() - df_jp_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_jp_clean['weekday'] = df_jp_clean['publish_time'].dt.day_name()\n",
        "df_jp_clean['hour'] = df_jp_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_jp_clean.to_csv(\"df_jp_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_jp_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_jp_clean)}\")\n"
      ],
      "metadata": {
        "id": "VLvtYYITr1va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "\n",
        "# โหลด CSV\n",
        "df_jp_clean = pd.read_csv(\"/content/df_jp_clean.csv\")\n",
        "\n",
        "# เลือก Top trending 100 แถว (ตาม views สูงสุด)\n",
        "df_jp_clean = df_jp_clean.sort_values(by='views', ascending=False).head(100)\n",
        "\n",
        "# สร้าง Translator\n",
        "translator = Translator()\n",
        "\n",
        "# ฟังก์ชันแปลภาษา พร้อมจัดการ error\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        return translator.translate(str(text), dest='en').text\n",
        "    except:\n",
        "        return str(text)\n",
        "\n",
        "# แปล title\n",
        "df_jp_clean['title_en'] = df_jp_clean['title'].apply(translate_text)\n",
        "\n",
        "# บันทึก CSV\n",
        "df_jp_clean.to_csv(\"/content/df_jp_clean.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "display(df_jp_clean.head(20))"
      ],
      "metadata": {
        "id": "Ze2wXpa_yehL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล Korea"
      ],
      "metadata": {
        "id": "khvgngK8GPpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube KR Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/KRvideos.csv\"\n",
        "\n",
        "# Read the CSV into df_kr\n",
        "\n",
        "df_kr = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_kr.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_kr.show(20)\n",
        "\n"
      ],
      "metadata": {
        "id": "KLICxtrZGwSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KR Cleaning"
      ],
      "metadata": {
        "id": "jUdA0kZhy_4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_kr.count()}\")"
      ],
      "metadata": {
        "id": "8LvX37W0zBuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "#กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_kr_clean = df_kr.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\")) &\n",
        "    (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_kr_clean = df_kr_clean.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_kr_clean = df_kr_clean.dropDuplicates()\n",
        "\n",
        "#จัดลำดับตาม views จากมากไปน้อย\n",
        "df_kr_clean = df_kr_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_kr_clean.show(20, truncate=False)\n",
        "\n",
        "#แปลงเป็น Pandas และบันทึกเป้น CSV\n",
        "df_kr_clean = df_kr_clean.toPandas()\n",
        "df_kr_clean.to_csv(\"df_kr_clean.csv\", index=False)\n",
        "\n",
        "#แสดงจำนวนแถว\n",
        "print(f\"จำนวนแถว: {df_kr_clean.count()}\")"
      ],
      "metadata": {
        "id": "PI2_fFn1zD6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube KR Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/KRvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_kr_clean_spark = df_kr.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_kr_clean = df_kr_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_kr_clean[c] = pd.to_numeric(df_kr_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_kr_clean['publish_time'] = pd.to_datetime(df_kr_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_kr_clean['trending_date'] = pd.to_datetime(df_kr_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_kr_clean['title_length'] = df_kr_clean['title'].astype(str).apply(len)\n",
        "df_kr_clean['tags_count'] = df_kr_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_kr_clean['days_to_trending'] = (df_kr_clean['trending_date'].dt.normalize() - df_kr_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_kr_clean['weekday'] = df_kr_clean['publish_time'].dt.day_name()\n",
        "df_kr_clean['hour'] = df_kr_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_kr_clean.to_csv(\"df_kr_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_kr_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_kr_clean)}\")\n"
      ],
      "metadata": {
        "id": "e3zeO8Gzs7Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล Mexico\n"
      ],
      "metadata": {
        "id": "0DDV4sZvHExf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube MX Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/MXvideos.csv\"\n",
        "\n",
        "# Read the CSV into df_mx\n",
        "\n",
        "df_mx = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_mx.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_mx.show(20)\n",
        "\n"
      ],
      "metadata": {
        "id": "awbWiZZfHLVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MX Cleaning"
      ],
      "metadata": {
        "id": "VCFp58150u-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_mx.count()}\")"
      ],
      "metadata": {
        "id": "SbH6ds4S0yuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "#กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_mx_clean = df_mx.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        "    & (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        "    & (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_mx_clean = df_mx.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_mx_clean = df_mx_clean.dropDuplicates()\n",
        "\n",
        "#จัดลำดับตาม views จากมากไปน้อย\n",
        "df_mx_clean = df_mx_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_mx_clean.show(20, truncate=False)\n",
        "\n",
        "#แปลงเป็น Pandas และบันทึกเป้น CSV\n",
        "df_mx_clean = df_mx_clean.toPandas()\n",
        "df_mx_clean.to_csv(\"df_mx_clean.csv\", index=False)\n",
        "\n",
        "#แสดงจำนวนแถว\n",
        "print(f\"จำนวนแถว: {df_mx_clean.count()}\")"
      ],
      "metadata": {
        "id": "jA5A-nR003HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube MX Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/MXvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_mx_clean_spark = df_mx.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_mx_clean = df_mx_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_mx_clean[c] = pd.to_numeric(df_mx_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_mx_clean['publish_time'] = pd.to_datetime(df_mx_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_mx_clean['trending_date'] = pd.to_datetime(df_mx_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_mx_clean['title_length'] = df_mx_clean['title'].astype(str).apply(len)\n",
        "df_mx_clean['tags_count'] = df_mx_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_mx_clean['days_to_trending'] = (df_mx_clean['trending_date'].dt.normalize() - df_mx_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_mx_clean['weekday'] = df_mx_clean['publish_time'].dt.day_name()\n",
        "df_mx_clean['hour'] = df_mx_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_mx_clean.to_csv(\"df_mx_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_mx_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_mx_clean)}\")\n"
      ],
      "metadata": {
        "id": "tjnmsod2txx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ข้อมูล Russian"
      ],
      "metadata": {
        "id": "JLUS4La0HYii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube RU Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ใส่ path ให้ตรงกับไฟล์ที่แตกไว้\n",
        "csv_path = \"/content/mypath/youtube-new/RUvideos.csv\"\n",
        "\n",
        "# Read the CSV into df_ru\n",
        "\n",
        "df_ru = spark.read.options(\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ").csv(csv_path)\n",
        "\n",
        "# ดู schema\n",
        "df_ru.printSchema()\n",
        "\n",
        "# แสดงตัวอย่างข้อมูล\n",
        "df_ru.show(20)\n",
        "\n"
      ],
      "metadata": {
        "id": "8BjzEfp2HfkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RU Cleaning"
      ],
      "metadata": {
        "id": "PBtS2Cdf1Lcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"จำนวนแถว: {df_ru.count()}\")"
      ],
      "metadata": {
        "id": "8JUD9qEl1Pn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "#กรองให้ views, likes, dislikes เป็นตัวเลขเท่านั้น\n",
        "df_ru_clean = df_ru.filter(\n",
        "    (col(\"views\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        "    & (col(\"likes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        "    & (col(\"dislikes\").cast(\"string\").rlike(\"^[0-9]+$\"))\n",
        ")\n",
        "\n",
        "# ลบแถวที่มีค่าว่างในทุกคอลัมน์\n",
        "df_ru_clean = df_ru_clean.dropna()\n",
        "\n",
        "# ลบแถวซ้ำทั้งหมด\n",
        "df_ru_clean = df_ru_clean.dropDuplicates()\n",
        "\n",
        "#จัดลำดับตาม views จากมากไปน้อย\n",
        "df_ru_clean = df_ru_clean.orderBy(col(\"views\").desc())\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "df_ru_clean.show(20, truncate=False)\n",
        "\n",
        "#แปลงเป็น Pandas และบันทึกเป้น CSV\n",
        "df_ru_clean = df_ru_clean.toPandas()\n",
        "df_ru_clean.to_csv(\"df_ru_clean.csv\", index=False)\n",
        "\n",
        "#แสดงจำนวนแถว\n",
        "print(f\"จำนวนแถว: {df_ru_clean.count()}\")"
      ],
      "metadata": {
        "id": "az9rzSGp1VEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YouTube RU Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path CSV\n",
        "csv_path = \"/content/mypath/youtube-new/RUvideos.csv\"\n",
        "\n",
        "# Read CSV\n",
        "# --- Clean ด้วย Spark ---\n",
        "df_ru_clean_spark = df_ru.filter(col(\"title\").rlike(\"[A-Za-z]\")) \\\n",
        "    .filter(col(\"views\").rlike(\"^[0-9]+$\") & col(\"likes\").rlike(\"^[0-9]+$\") & col(\"dislikes\").rlike(\"^[0-9]+$\")) \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"views\").desc())\n",
        "\n",
        "# --- แปลงเป็น Pandas ---\n",
        "df_ru_clean = df_ru_clean_spark.toPandas()\n",
        "\n",
        "# แปลงคอลัมน์ตัวเลข\n",
        "for c in ['views', 'likes', 'dislikes', 'comment_count']:\n",
        "    df_ru_clean[c] = pd.to_numeric(df_ru_clean[c], errors='coerce')\n",
        "\n",
        "# แปลง datetime\n",
        "df_ru_clean['publish_time'] = pd.to_datetime(df_ru_clean['publish_time'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "df_ru_clean['trending_date'] = pd.to_datetime(df_ru_clean['trending_date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "\n",
        "# เพิ่มคอลัมน์ใหม่\n",
        "df_ru_clean['title_length'] = df_ru_clean['title'].astype(str).apply(len)\n",
        "df_ru_clean['tags_count'] = df_ru_clean['tags'].astype(str).apply(lambda x: 0 if x.strip().lower() == \"[none]\" else len(x.split(\"|\")))\n",
        "df_ru_clean['days_to_trending'] = (df_ru_clean['trending_date'].dt.normalize() - df_ru_clean['publish_time'].dt.normalize()).dt.days.astype('Int64')\n",
        "df_ru_clean['weekday'] = df_ru_clean['publish_time'].dt.day_name()\n",
        "df_ru_clean['hour'] = df_ru_clean['publish_time'].dt.hour\n",
        "\n",
        "# Save CSV\n",
        "df_ru_clean.to_csv(\"df_ru_ready.csv\", index=False)\n",
        "\n",
        "# ดูตัวอย่างผลลัพธ์\n",
        "print(df_ru_clean.head(20))\n",
        "\n",
        "# จำนวนแถว\n",
        "print(f\"จำนวนแถว: {len(df_ru_clean)}\")\n"
      ],
      "metadata": {
        "id": "OcqsmH0Zulea"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}